{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"driftbench","text":"<p>This is the documentation of <code>driftbench</code>, a framwork to synthetically generate process curves and to benchmark process drift detectors.</p> <p>Please consider citing if you use <code>driftbench</code> in your research:</p> <pre><code>@article{wolf_method_2025,\n    title = {A method to benchmark high-dimensional process drift detection},\n    issn = {1572-8145},\n    url = {https://doi.org/10.1007/s10845-025-02590-9},\n    doi = {10.1007/s10845-025-02590-9},\n    journal = {Journal of Intelligent Manufacturing},\n    author = {Wolf, Edgar and Windisch, Tobias},\n    year = {2025},\n}\n</code></pre>"},{"location":"index.html#about","title":"About","text":"<p><code>driftbench</code> is a benchmarking framework for detecting drifts in high-dimensional process curve data, commonly found in industrial manufacturing. Process curves are multivariate time series generated by repeated executions of manufacturing steps (e.g., pressing, screwing).</p> <p>The key contributions of this package the following:</p>"},{"location":"index.html#synthetic-data-generation-framework","title":"Synthetic Data Generation Framework","text":"<p>A statistical generative model to create synthetic process curves with controllable drift behavior. Each process curve is modeled as a function \\(f(w(t), x)\\), where \\(w(t)\\) are latent parameters evolving over time. Drifts are injected by moving \u201csupport points\u201d (e.g., maxima, inflection points) in the curves over time. Parameters \\(w(t)\\) are optimized to satisfy these support point constraints, allowing for highly controlled and realistic drift patterns.</p>"},{"location":"index.html#a-novel-temporal-evaluation-metric","title":"A novel Temporal Evaluation Metric","text":"<p>We introduce a metric called Temporal Area Under the Curve (TAUC) that evaluates how well drift detectors capture drift segments in their temporal context. Unlike standard AUC, TAUC rewards detectors that correctly identify the onset, duration, and position of drifts, rather than just individual anomalous points. A soft version (sTAUC) is also introduced, accounting for partial overlaps between detected and true drift segments.</p>"},{"location":"index.html#benchmarking-functionality","title":"Benchmarking functionality","text":"<p>This package provides several common drift detection techniques (autoencoder-based, clustering, statistical tests like KS or MMD) which can be evaluated on synthetic datasets with different types and numbers of drift segments.</p>"},{"location":"index.html#getting-started","title":"Getting started","text":"<p>This is a minimal example to generate <code>N=10</code> curves from a cubic function:</p> <pre><code>import numpy as np\nfrom driftbench.data_generation.loaders import load_dataset_specification_from_yaml\nfrom driftbench.data_generation.sample import sample_curves\n\ninput = \"\"\"\nexample:\n  N: 10\n  dimensions: 10\n  latent_information:\n    !LatentInformation\n    y0: [0, 8, 64]\n    x0: [0, 2, 4]\n    y1: [3, 27]\n    x1: [1, 3]\n    y2: [12]\n    x2: [2]\n  drifts:\n    !DriftSequence\n      - !LinearDrift\n        start: 3\n        end: 5\n        feature: x0\n        dimension: 1\n        m: 0.1\n\"\"\"\n\ndef f(w, x):\n    return w[0] * x ** 3 + w[1] * x ** 2 + w[2] * x + w[3]\n\nw0 = np.zeros(4)\ndataset = load_dataset_specification_from_yaml(input)\ncoefficients, latent_information, curves = sample_curves(dataset[\"example\"], w0=w0, f=f)\n</code></pre>"},{"location":"api.html","title":"Data generation","text":""},{"location":"api.html#driftbench.data_generation.sample.sample_curves","title":"<code>sample_curves(dataset_specification, f=None, w0=None, random_state=2024, measurement_scale=None, callback=None)</code>","text":"<p>Samples synthetic curves given a dataset specification.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_specification</code> <code>dict</code> <p>A dataset specification which contains</p> required <code>f</code> <code>Callable</code> <p>The function to fit the curves. Use this parameter if no function is specified</p> <code>None</code> <code>w0</code> <code>ndarray</code> <p>The inital guess for the optimization problem used to synthesize curves.</p> <code>None</code> <code>random_state</code> <code>int</code> <p>The random state for reproducablity.</p> <code>2024</code> <code>measurement_scale</code> <code>float</code> <p>The scale for the noise applied on the evaluated curves. If not</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The coefficients for each sampled curve.</p> <code>list[LatentInformation]</code> <p>The latent information for each sampled curve.</p> <code>ndarray</code> <p>The evaluated sampled curves.</p> Source code in <code>driftbench/data_generation/sample.py</code> <pre><code>def sample_curves(\n    dataset_specification,\n    f=None,\n    w0=None,\n    random_state=2024,\n    measurement_scale=None,\n    callback=None,\n):\n    \"\"\"\n    Samples synthetic curves given a dataset specification.\n\n    Args:\n        dataset_specification (dict): A dataset specification which contains\n        all information to syntethisize curves in yaml-format.\n        Each dataset is encoded with a name and needs a latent information provided.\n        The function `f` to fit and as well as initial guess `w0`can be provided as well.\n        f (Callable): The function to fit the curves. Use this parameter if no function is specified\n        in `dataset_specification`.\n        w0 (np.ndarray): The inital guess for the optimization problem used to synthesize curves.\n        Use this parameter if no initial guess is specified in `dataset_specification`.\n        random_state (int): The random state for reproducablity.\n        measurement_scale (float): The scale for the noise applied on the evaluated curves. If not\n        set, 5% percent of the mean of the curves is used. Set to 0.0 if you want to omit\n        this noise.\n\n    Returns:\n        (np.ndarray): The coefficients for each sampled curve.\n        (list[LatentInformation]): The latent information for each sampled curve.\n        (np.ndarray): The evaluated sampled curves.\n    \"\"\"\n    dimensions = dataset_specification[\"dimensions\"]\n    drifts = dataset_specification.get(\"drifts\")\n    x_scale = dataset_specification.get(\"x_scale\", 0.02)\n    y_scale = dataset_specification.get(\"y_scale\", 0.1)\n    func = _get_func(dataset_specification, f)\n    w_init = _get_w_init(dataset_specification, w0)\n    rng = np.random.RandomState(random_state)\n    latent_information = _generate_latent_information(\n        dataset_specification, rng, x_scale, y_scale\n    )\n    if drifts is not None:\n        latent_information = drifts.apply(latent_information)\n    data_generator = CurveGenerator(func, w_init)\n    w = data_generator.run(latent_information, callback=callback)\n    x_range = np.concatenate(\n        (\n            dataset_specification[\"latent_information\"].x0,\n            dataset_specification[\"latent_information\"].x1,\n            dataset_specification[\"latent_information\"].x2,\n        )\n    )\n    x_min, x_max = int(np.min(x_range)), int(np.max(x_range))\n    xs = np.linspace(x_min, x_max, dimensions)\n    curves = np.array([func(w_i, xs) for w_i in w])\n    # Apply a default noise of 5% of the mean of the sampled curves\n    if measurement_scale is None:\n        scale = 0.05 * np.mean(curves)\n        curves = rng.normal(loc=curves, scale=scale)\n    else:\n        curves = rng.normal(loc=curves, scale=measurement_scale)\n    return w, latent_information, curves\n</code></pre>"},{"location":"api.html#driftbench.data_generation.drifts.Drift","title":"<code>Drift</code>","text":"<p>Represents a drift for 1d or 2d input.</p> Source code in <code>driftbench/data_generation/drifts.py</code> <pre><code>class Drift(metaclass=ABCMeta):\n    \"\"\"\n    Represents a drift for 1d or 2d input.\n    \"\"\"\n\n    def __init__(self, start, end, feature=None, dimension=0) -&gt; None:\n        \"\"\"\n        Args:\n            start (int): The start index.\n            end (int): The end index.\n            feature (str): The feature the drift should be applied on.\n            dimension (int): The dimension the drift should be applied on.\n        \"\"\"\n        self._validate_drift_bounds(start, end)\n        self.start = start\n        self.end = end\n        self.feature = feature\n        self.dimension = dimension\n\n    def _validate_drift_bounds(self, start, end):\n        if start &gt;= end:\n            raise ValueError(\"End must be greater than start.\")\n        if start &lt; 0 or end &lt; 0:\n            raise ValueError(\"Drift bounds are not allowed to be negative.\")\n\n    @abstractmethod\n    def transform(self, X):\n        \"\"\"\n        Applies the transformation specified by the drift object on the given input\n        Args:\n            X (numpy.ndarray): The 1d- or 2d-input data to be drifted.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api.html#driftbench.data_generation.drifts.Drift.__init__","title":"<code>__init__(start, end, feature=None, dimension=0)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>The start index.</p> required <code>end</code> <code>int</code> <p>The end index.</p> required <code>feature</code> <code>str</code> <p>The feature the drift should be applied on.</p> <code>None</code> <code>dimension</code> <code>int</code> <p>The dimension the drift should be applied on.</p> <code>0</code> Source code in <code>driftbench/data_generation/drifts.py</code> <pre><code>def __init__(self, start, end, feature=None, dimension=0) -&gt; None:\n    \"\"\"\n    Args:\n        start (int): The start index.\n        end (int): The end index.\n        feature (str): The feature the drift should be applied on.\n        dimension (int): The dimension the drift should be applied on.\n    \"\"\"\n    self._validate_drift_bounds(start, end)\n    self.start = start\n    self.end = end\n    self.feature = feature\n    self.dimension = dimension\n</code></pre>"},{"location":"api.html#driftbench.data_generation.drifts.Drift.transform","title":"<code>transform(X)</code>  <code>abstractmethod</code>","text":"<p>Applies the transformation specified by the drift object on the given input Args:     X (numpy.ndarray): The 1d- or 2d-input data to be drifted.</p> Source code in <code>driftbench/data_generation/drifts.py</code> <pre><code>@abstractmethod\ndef transform(self, X):\n    \"\"\"\n    Applies the transformation specified by the drift object on the given input\n    Args:\n        X (numpy.ndarray): The 1d- or 2d-input data to be drifted.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api.html#driftbench.data_generation.drifts.DriftSequence","title":"<code>DriftSequence</code>","text":"<p>Represents a sequence of drifts, which will be applied on a latent information object.</p> Source code in <code>driftbench/data_generation/drifts.py</code> <pre><code>class DriftSequence:\n    \"\"\"\n    Represents a sequence of drifts, which will be applied on a latent information object.\n    \"\"\"\n\n    def __init__(self, drifts):\n        \"\"\"\n        Args:\n            drifts (list[Drift]): A list of drifts which are being used for the transformation.\n        \"\"\"\n        self._validate_drifts(drifts)\n        self.drifts = sorted(drifts, key=lambda drift: drift.start)\n\n    def apply(self, X):\n        \"\"\"\n        Applies the transformation by the given drifts on the latent information input.\n\n        Args:\n            X (list[LatentInformation]): The list of latent information the drifts are applied on.\n\n        Returns:\n            (list[LatentInformation]): A list of drifted latent information according to the drift sequence.\n        \"\"\"\n        drifted = copy.deepcopy(X)\n        for drift in self.drifts:\n            feature = np.array([getattr(x, drift.feature) for x in drifted])\n            feature[:, drift.dimension] = drift.transform(\n                feature[:, drift.dimension]\n            ).flatten()\n            for i, x in enumerate(drifted):\n                setattr(x, drift.feature, feature[i])\n        return drifted\n\n    def get_aggregated_drift_bounds(self):\n        \"\"\"\n        Returns the aggregated drift bounds, i.e. the maximum range where drifts are applied.\n\n        Returns:\n            (tuple[int, int]): A tuple of (int, int), where the first value denotes the start\n            index and the second value the end index of the aggregated drift bounds.\n        \"\"\"\n        start = self.drifts[0].start\n        end = self.drifts[-1].end\n        return start, end\n\n    def get_individual_drift_bounds(self):\n        \"\"\"\n        Returns the drift bounds for each individual drift in the drift sequence.\n\n        Returns:\n            (list[tuple[int, int]]): A list of tuples of (int, int), where the first value denotes\n            the start of the drift, and the second value the end of the drift.\n        \"\"\"\n        return [(drift.start, drift.end) for drift in self.drifts]\n\n    def get_drift_intensities(self):\n        \"\"\"\n        Returns the intensities for each range in the drift sequence. Each drift has a base intensity of 1,\n        and when multiple drifts overlap, the intensity becomes the number of the drifts present in the given\n        range.\n\n        Returns:\n            (dict[tuple[int, int], int]): A dictionary with tuples as keys and ints as values.\n            The keys indicate the range of the drift intensity, and the values indicate the intensity.\n        \"\"\"\n        intensities = {}\n        drift_intensities_array = np.zeros(\n            (len(self.drifts), np.max([drift.end for drift in self.drifts]) + 1)\n        )\n        for i, drift in enumerate(self.drifts):\n            drift_intensities_array[i, drift.start : drift.end + 1] = 1\n        stacked_drift_intensities = np.sum(drift_intensities_array, axis=0)\n\n        for intensity in range(1, np.max(stacked_drift_intensities).astype(int) + 1):\n            indices = np.where(stacked_drift_intensities == intensity)[0]\n            split_indices = np.where(np.diff(indices) &gt; 1)[0] + 1\n            bounds = np.split(indices, split_indices)\n            for start, end in [(bound[0], bound[-1]) for bound in bounds]:\n                intensities[(start, end)] = intensity\n        return intensities\n\n    def _validate_drifts(self, drifts):\n        # Group drifts by their feature and their dimension they apply on.\n        drifts_sorted = sorted(\n            drifts, key=lambda drift: (drift.feature, drift.dimension)\n        )\n        drifts_grouped = groupby(\n            drifts_sorted, key=lambda drift: (drift.feature, drift.dimension)\n        )\n        # Check within these groups if an overlap exists.\n        for (feature, dimension), curr_drifts in drifts_grouped:\n            curr_drifts = list(curr_drifts)\n            for i, j in combinations(range(len(curr_drifts)), 2):\n                drift1 = curr_drifts[i]\n                drift2 = curr_drifts[j]\n                if drift1.start &lt;= drift2.end and drift2.start &lt;= drift1.end:\n                    raise ValueError(\n                        f\"Drifts are not allowed to overlap. \"\n                        f\"Overlapping drift at feature {feature} in dimension {dimension}\"\n                    )\n</code></pre>"},{"location":"api.html#driftbench.data_generation.drifts.DriftSequence.__init__","title":"<code>__init__(drifts)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>drifts</code> <code>list[Drift]</code> <p>A list of drifts which are being used for the transformation.</p> required Source code in <code>driftbench/data_generation/drifts.py</code> <pre><code>def __init__(self, drifts):\n    \"\"\"\n    Args:\n        drifts (list[Drift]): A list of drifts which are being used for the transformation.\n    \"\"\"\n    self._validate_drifts(drifts)\n    self.drifts = sorted(drifts, key=lambda drift: drift.start)\n</code></pre>"},{"location":"api.html#driftbench.data_generation.drifts.DriftSequence.apply","title":"<code>apply(X)</code>","text":"<p>Applies the transformation by the given drifts on the latent information input.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>list[LatentInformation]</code> <p>The list of latent information the drifts are applied on.</p> required <p>Returns:</p> Type Description <code>list[LatentInformation]</code> <p>A list of drifted latent information according to the drift sequence.</p> Source code in <code>driftbench/data_generation/drifts.py</code> <pre><code>def apply(self, X):\n    \"\"\"\n    Applies the transformation by the given drifts on the latent information input.\n\n    Args:\n        X (list[LatentInformation]): The list of latent information the drifts are applied on.\n\n    Returns:\n        (list[LatentInformation]): A list of drifted latent information according to the drift sequence.\n    \"\"\"\n    drifted = copy.deepcopy(X)\n    for drift in self.drifts:\n        feature = np.array([getattr(x, drift.feature) for x in drifted])\n        feature[:, drift.dimension] = drift.transform(\n            feature[:, drift.dimension]\n        ).flatten()\n        for i, x in enumerate(drifted):\n            setattr(x, drift.feature, feature[i])\n    return drifted\n</code></pre>"},{"location":"api.html#driftbench.data_generation.drifts.DriftSequence.get_aggregated_drift_bounds","title":"<code>get_aggregated_drift_bounds()</code>","text":"<p>Returns the aggregated drift bounds, i.e. the maximum range where drifts are applied.</p> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>A tuple of (int, int), where the first value denotes the start</p> <p>index and the second value the end index of the aggregated drift bounds.</p> Source code in <code>driftbench/data_generation/drifts.py</code> <pre><code>def get_aggregated_drift_bounds(self):\n    \"\"\"\n    Returns the aggregated drift bounds, i.e. the maximum range where drifts are applied.\n\n    Returns:\n        (tuple[int, int]): A tuple of (int, int), where the first value denotes the start\n        index and the second value the end index of the aggregated drift bounds.\n    \"\"\"\n    start = self.drifts[0].start\n    end = self.drifts[-1].end\n    return start, end\n</code></pre>"},{"location":"api.html#driftbench.data_generation.drifts.DriftSequence.get_drift_intensities","title":"<code>get_drift_intensities()</code>","text":"<p>Returns the intensities for each range in the drift sequence. Each drift has a base intensity of 1, and when multiple drifts overlap, the intensity becomes the number of the drifts present in the given range.</p> <p>Returns:</p> Type Description <code>dict[tuple[int, int], int]</code> <p>A dictionary with tuples as keys and ints as values.</p> <p>The keys indicate the range of the drift intensity, and the values indicate the intensity.</p> Source code in <code>driftbench/data_generation/drifts.py</code> <pre><code>def get_drift_intensities(self):\n    \"\"\"\n    Returns the intensities for each range in the drift sequence. Each drift has a base intensity of 1,\n    and when multiple drifts overlap, the intensity becomes the number of the drifts present in the given\n    range.\n\n    Returns:\n        (dict[tuple[int, int], int]): A dictionary with tuples as keys and ints as values.\n        The keys indicate the range of the drift intensity, and the values indicate the intensity.\n    \"\"\"\n    intensities = {}\n    drift_intensities_array = np.zeros(\n        (len(self.drifts), np.max([drift.end for drift in self.drifts]) + 1)\n    )\n    for i, drift in enumerate(self.drifts):\n        drift_intensities_array[i, drift.start : drift.end + 1] = 1\n    stacked_drift_intensities = np.sum(drift_intensities_array, axis=0)\n\n    for intensity in range(1, np.max(stacked_drift_intensities).astype(int) + 1):\n        indices = np.where(stacked_drift_intensities == intensity)[0]\n        split_indices = np.where(np.diff(indices) &gt; 1)[0] + 1\n        bounds = np.split(indices, split_indices)\n        for start, end in [(bound[0], bound[-1]) for bound in bounds]:\n            intensities[(start, end)] = intensity\n    return intensities\n</code></pre>"},{"location":"api.html#driftbench.data_generation.drifts.DriftSequence.get_individual_drift_bounds","title":"<code>get_individual_drift_bounds()</code>","text":"<p>Returns the drift bounds for each individual drift in the drift sequence.</p> <p>Returns:</p> Type Description <code>list[tuple[int, int]]</code> <p>A list of tuples of (int, int), where the first value denotes</p> <p>the start of the drift, and the second value the end of the drift.</p> Source code in <code>driftbench/data_generation/drifts.py</code> <pre><code>def get_individual_drift_bounds(self):\n    \"\"\"\n    Returns the drift bounds for each individual drift in the drift sequence.\n\n    Returns:\n        (list[tuple[int, int]]): A list of tuples of (int, int), where the first value denotes\n        the start of the drift, and the second value the end of the drift.\n    \"\"\"\n    return [(drift.start, drift.end) for drift in self.drifts]\n</code></pre>"},{"location":"api.html#driftbench.data_generation.drifts.LinearDrift","title":"<code>LinearDrift</code>","text":"<p>               Bases: <code>Drift</code></p> <p>Represents a linear drift for a 1d or 2d-input, i.e. a drift where the input data is drifted in a linear fashion.</p> Source code in <code>driftbench/data_generation/drifts.py</code> <pre><code>class LinearDrift(Drift):\n    \"\"\"\n    Represents a linear drift for a 1d or 2d-input, i.e. a drift\n    where the input data is drifted in a linear fashion.\n    \"\"\"\n\n    def __init__(self, start, end, m, feature=None, dimension=0):\n        \"\"\"\n        Args:\n            start (int): The start index.\n            end (int): The end index.\n            m (float): The slope of the linear drift. Usually in the range (-1, 1)\n            feature (str): The feature the drift should be applied on.\n            dimension (int): The dimension the drift should be applied on.\n        \"\"\"\n        super().__init__(start, end, feature=feature, dimension=dimension)\n        self.m = m\n\n    def transform(self, X):\n        drifted = np.copy(X).astype(float)\n        if drifted.ndim == 1:\n            drifted = drifted.reshape(-1, 1)\n        # Use 0 based x indices for computing the slope at a given position\n        xs = np.arange(self.end - self.start + 1).reshape(-1, 1)\n        drifted[self.start : self.end + 1, :] += self.m * xs\n        # Maintain data according to new data after drift happened.\n        after_drift_idx = drifted.shape[0] - self.end\n        drifted[-after_drift_idx + 1 :, :] += self.m * xs[-1]\n        return drifted\n</code></pre>"},{"location":"api.html#driftbench.data_generation.drifts.LinearDrift.__init__","title":"<code>__init__(start, end, m, feature=None, dimension=0)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>The start index.</p> required <code>end</code> <code>int</code> <p>The end index.</p> required <code>m</code> <code>float</code> <p>The slope of the linear drift. Usually in the range (-1, 1)</p> required <code>feature</code> <code>str</code> <p>The feature the drift should be applied on.</p> <code>None</code> <code>dimension</code> <code>int</code> <p>The dimension the drift should be applied on.</p> <code>0</code> Source code in <code>driftbench/data_generation/drifts.py</code> <pre><code>def __init__(self, start, end, m, feature=None, dimension=0):\n    \"\"\"\n    Args:\n        start (int): The start index.\n        end (int): The end index.\n        m (float): The slope of the linear drift. Usually in the range (-1, 1)\n        feature (str): The feature the drift should be applied on.\n        dimension (int): The dimension the drift should be applied on.\n    \"\"\"\n    super().__init__(start, end, feature=feature, dimension=dimension)\n    self.m = m\n</code></pre>"},{"location":"api.html#driftbench.data_generation.solvers.JaxCurveGenerationSolver","title":"<code>JaxCurveGenerationSolver</code>","text":"<p>               Bases: <code>Solver</code></p> <p>Fits latent information according to a given polynomial.</p> Source code in <code>driftbench/data_generation/solvers.py</code> <pre><code>class JaxCurveGenerationSolver(Solver):\n    \"\"\"\n    Fits latent information according to a given polynomial.\n    \"\"\"\n\n    def __init__(self, p, w0, max_fit_attemps, random_seed):\n        \"\"\"\n        Args:\n            p (Callable): The polynomial.\n            w0 (list-like): The initial guess for the solution.\n            max_fit_attemps (int): The maxmium number of attempts to refit a curve, if optimization didn't succeed.\n            random_seed (int): The random seed for the random number generator.\n        \"\"\"\n        self.p = p\n        self.dp_dx = grad(p, argnums=1)\n        self.dp_dx2 = grad(self.dp_dx, argnums=1)\n        self.w0 = jnp.array(w0)\n        self.max_fit_attempts = max_fit_attemps\n        self.rng = np.random.RandomState(random_seed)\n\n    def solve(self, X, callback=None):\n        coefficients = []\n        p = jit(vmap(partial(self.p), in_axes=(None, 0)))\n        dp_dx = jit(vmap(partial(self.dp_dx), in_axes=(None, 0)))\n        dp_dx2 = jit(vmap(partial(self.dp_dx2), in_axes=(None, 0)))\n        solution = self.w0\n        for i, latent in enumerate(X):\n            result = _minimize(\n                p,\n                dp_dx,\n                dp_dx2,\n                solution,\n                latent.y0,\n                latent.x0,\n                latent.y1,\n                latent.x1,\n                latent.y2,\n                latent.x2,\n            )\n            if not result.success:\n                result = self._refit(p, dp_dx, dp_dx2, latent)\n            solution = result.x\n            if callback:\n                jax.debug.callback(callback, i, solution)\n            coefficients.append(solution)\n        return jnp.array(coefficients)\n\n    def _refit(self, p, dp_dx, dp_dx2, latent):\n        # Restart with initial guess in order to be independent of previous solutions.\n        solution = self.w0\n        current_fit_attempts = 0\n        success = False\n        result = None\n        # Fallback strategy: If fit is not successful, try again and use previous solution\n        # for the same problem as starting point until convergence.\n        while not success and current_fit_attempts &lt; self.max_fit_attempts:\n            current_fit_attempts += 1\n            result = _minimize(\n                p,\n                dp_dx,\n                dp_dx2,\n                solution,\n                latent.y0,\n                latent.x0,\n                latent.y1,\n                latent.x1,\n                latent.y2,\n                latent.x2,\n            )\n            solution = result.x\n            success = result.success\n        return result\n</code></pre>"},{"location":"api.html#driftbench.data_generation.solvers.JaxCurveGenerationSolver.__init__","title":"<code>__init__(p, w0, max_fit_attemps, random_seed)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>p</code> <code>Callable</code> <p>The polynomial.</p> required <code>w0</code> <code>list - like</code> <p>The initial guess for the solution.</p> required <code>max_fit_attemps</code> <code>int</code> <p>The maxmium number of attempts to refit a curve, if optimization didn't succeed.</p> required <code>random_seed</code> <code>int</code> <p>The random seed for the random number generator.</p> required Source code in <code>driftbench/data_generation/solvers.py</code> <pre><code>def __init__(self, p, w0, max_fit_attemps, random_seed):\n    \"\"\"\n    Args:\n        p (Callable): The polynomial.\n        w0 (list-like): The initial guess for the solution.\n        max_fit_attemps (int): The maxmium number of attempts to refit a curve, if optimization didn't succeed.\n        random_seed (int): The random seed for the random number generator.\n    \"\"\"\n    self.p = p\n    self.dp_dx = grad(p, argnums=1)\n    self.dp_dx2 = grad(self.dp_dx, argnums=1)\n    self.w0 = jnp.array(w0)\n    self.max_fit_attempts = max_fit_attemps\n    self.rng = np.random.RandomState(random_seed)\n</code></pre>"},{"location":"api.html#driftbench.data_generation.solvers.Solver","title":"<code>Solver</code>","text":"<p>Represents a backend for solving an optimization problem.</p> Source code in <code>driftbench/data_generation/solvers.py</code> <pre><code>class Solver(metaclass=ABCMeta):\n    \"\"\"\n    Represents a backend for solving an optimization problem.\n    \"\"\"\n\n    @abstractmethod\n    def solve(self, X):\n        \"\"\"\n        Solves an optimization problem defined by the solver.\n\n        Args:\n            X (list-like): Input to optimize according to solver instance.\n\n        Returns:\n            (np.ndarray|jnp.ndarray): The parameters obtained by solving the optimzation problem.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api.html#driftbench.data_generation.solvers.Solver.solve","title":"<code>solve(X)</code>  <code>abstractmethod</code>","text":"<p>Solves an optimization problem defined by the solver.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>list - like</code> <p>Input to optimize according to solver instance.</p> required <p>Returns:</p> Type Description <code>ndarray | ndarray</code> <p>The parameters obtained by solving the optimzation problem.</p> Source code in <code>driftbench/data_generation/solvers.py</code> <pre><code>@abstractmethod\ndef solve(self, X):\n    \"\"\"\n    Solves an optimization problem defined by the solver.\n\n    Args:\n        X (list-like): Input to optimize according to solver instance.\n\n    Returns:\n        (np.ndarray|jnp.ndarray): The parameters obtained by solving the optimzation problem.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api.html#driftbench.data_generation.latent_information.LatentInformation","title":"<code>LatentInformation</code>  <code>dataclass</code>","text":"<p>Represents the local latent information for high-dimensional object, which is used to generate such high-dimensional data. Currently, this structure is designed for creating curves meeting the conditions provided by the attributes defined in this class.</p> <p>Parameters:</p> Name Type Description Default <code>y0</code> <code>list - like</code> <p>The y-values of a function.</p> required <code>x0</code> <code>list - like</code> <p>The x-values of a function. Hence, no duplicates are allowed.</p> required <code>y1</code> <code>list - like</code> <p>The y-values of the derivative of a function.</p> required <code>x1</code> <code>list - like</code> <p>The x-values of the derivative of a function.</p> required <code>y2</code> <code>list - like</code> <p>The y-values of the derivative of a function.</p> required <code>x2</code> <code>list - like</code> <p>The x-values of the second derivative of a function.</p> required Source code in <code>driftbench/data_generation/latent_information.py</code> <pre><code>@dataclass\nclass LatentInformation:\n    \"\"\"\n    Represents the local latent information for high-dimensional object,\n    which is used to generate such high-dimensional data. Currently, this\n    structure is designed for creating curves meeting the conditions provided\n    by the attributes defined in this class.\n\n    Args:\n        y0 (list-like): The y-values of a function.\n        x0 (list-like): The x-values of a function. Hence, no duplicates are allowed.\n        y1 (list-like): The y-values of the derivative of a function.\n        x1 (list-like): The x-values of the derivative of a function.\n        Hence, no duplicates are allowed.\n        y2 (list-like): The y-values of the derivative of a function.\n        x2 (list-like): The x-values of the second derivative of a function.\n        Hence, no duplicates are allowed.\n    \"\"\"\n\n    y0: np.ndarray\n    x0: np.ndarray\n    y1: np.ndarray\n    x1: np.ndarray\n    y2: np.ndarray\n    x2: np.ndarray\n\n    def __post_init__(self):\n        self._validate_duplicates()\n        self._validate_1d_array()\n        self._validate_matching_shapes()\n\n    def _validate_matching_shapes(self):\n        if self.y0.shape != self.x0.shape:\n            raise ValueError(\n                \"Features y0 and x0 are not allowed to have different shape\"\n            )\n        if self.y1.shape != self.x1.shape:\n            raise ValueError(\n                \"Features y1 and x1 are not allowed to have different shape\"\n            )\n        if self.y2.shape != self.x2.shape:\n            raise ValueError(\n                \"Features y2 and x2 are not allowed to have different shape\"\n            )\n\n    def _validate_1d_array(self):\n\n        if self.y0.ndim != 1:\n            raise ValueError(\"Feature y0 has to be 1d-array.\")\n        if self.x0.ndim != 1:\n            raise ValueError(\"Feature x0 has to be 1d-array.\")\n        if self.y1.ndim != 1:\n            raise ValueError(\"Feature y1 has to be 1d-array.\")\n        if self.x1.ndim != 1:\n            raise ValueError(\"Feature x1 has to be 1d-array.\")\n        if self.y2.ndim != 1:\n            raise ValueError(\"Feature y2 has to be 1d-array.\")\n        if self.x2.ndim != 1:\n            raise ValueError(\"Feature x2 has to be 1d-array.\")\n\n    def _validate_duplicates(self):\n        _, x0_counts = np.unique(self.x0, return_counts=True)\n        _, x1_counts = np.unique(self.x1, return_counts=True)\n        _, x2_counts = np.unique(self.x2, return_counts=True)\n        if np.any(x0_counts &gt; 1):\n            raise ValueError(\"Feature x0 is not allowed to contain duplicates.\")\n        if np.any(x1_counts &gt; 1):\n            raise ValueError(\"Feature x1 is not allowed to contain duplicates.\")\n        if np.any(x2_counts &gt; 1):\n            raise ValueError(\"Feature x2 is not allowed to contain duplicates.\")\n</code></pre>"},{"location":"api.html#drift-detection","title":"Drift detection","text":"<p>The metrics module.</p>"},{"location":"api.html#driftbench.drift_detection.detectors.AggregateFeatureAlgorithm","title":"<code>AggregateFeatureAlgorithm</code>","text":"<p>               Bases: <code>Detector</code></p> <p>Detector that aggregates features over temporal axis.</p> Source code in <code>driftbench/drift_detection/detectors.py</code> <pre><code>class AggregateFeatureAlgorithm(Detector):\n    \"\"\"Detector that aggregates features over temporal axis.\n    \"\"\"\n    def __init__(self, agg_feature_func=None, algorithm=None):\n        self.algorithm = algorithm\n\n        if agg_feature_func is None:\n            agg_feature_func = np.mean\n        self.agg_feature_func = agg_feature_func\n\n    def predict(self, X):\n        input_with_feature = np.apply_along_axis(self.agg_feature_func, 1, X)\n        return self.algorithm.predict(input_with_feature)\n\n    @property\n    def name(self):\n        return self.algorithm.name\n</code></pre>"},{"location":"api.html#driftbench.drift_detection.detectors.AlwaysGuessDriftDetector","title":"<code>AlwaysGuessDriftDetector</code>","text":"<p>               Bases: <code>Detector</code></p> <p>A baseline detector.</p> Source code in <code>driftbench/drift_detection/detectors.py</code> <pre><code>class AlwaysGuessDriftDetector(Detector):\n    \"\"\"A baseline detector.\"\"\"\n    def predict(self, X):\n        return np.ones(X.shape[0])\n</code></pre>"},{"location":"api.html#driftbench.drift_detection.detectors.AutoencoderDetector","title":"<code>AutoencoderDetector</code>","text":"<p>               Bases: <code>Detector</code>, <code>Module</code></p> <p>Parameters:</p> Name Type Description Default <code>hidden_layers</code> <code>list</code> <p>List of number of neurons in each layer after input of encoder</p> required <code>retrain_always</code> <code>bool</code> <p>If true, model is always retrained when predict is called.</p> <code>False</code> Source code in <code>driftbench/drift_detection/detectors.py</code> <pre><code>class AutoencoderDetector(Detector, nn.Module):\n    \"\"\"\n\n    Args:\n        hidden_layers (list): List of number of neurons in each layer after input of encoder\n        retrain_always (bool): If true, model is always retrained when predict is called.\n    \"\"\"\n    _activation_functions = {\n        \"relu\": nn.ReLU(),\n        \"sigmoid\": nn.Sigmoid(),\n        \"tanh\": nn.Tanh(),\n    }\n\n    _optimizers = {\n        \"adam\": optim.Adam,\n        \"sgd\": optim.SGD,\n    }\n\n    _device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\n    hparams = ['activation', 'lr', 'optim', 'num_epochs', 'hidden_layers']\n\n    def __init__(self, hidden_layers, detector, activation='tanh', num_epochs=10,\n                 batch_size=32, optim=\"adam\", lr=0.001, retrain_always=False):\n        Detector.__init__(self)\n        nn.Module.__init__(self)\n\n        self.lr = lr\n        self.retrain = retrain_always\n        self.activation = activation\n        self.detector = detector\n        self.optim = optim\n        self.criterion = nn.MSELoss()\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.losses = []\n        self.hidden_layers = hidden_layers\n        self.is_trained = False\n        self.scaler = MinMaxScaler(feature_range=(-1, 1))\n\n    def forward(self, x):\n        latent_space = self.encoder(x)\n        reconstructed_x = self.decoder(latent_space)\n        return reconstructed_x\n\n    def _build_model(self, input_size):\n        encoder_layers = [input_size] + self.hidden_layers\n        decoder_layers = self.hidden_layers[::-1] + [input_size]\n        self.encoder = self._build_encoder(encoder_layers, self.activation)\n        self.decoder = self._build_decoder(decoder_layers, self.activation)\n        self.optimizer = self._optimizers[self.optim](self.parameters(), lr=self.lr)\n        self.to(self._device)\n\n    def _train(self, X):\n        # Reset losses from possible previous training\n        self.losses = []\n        # Actual training loop\n        dataset = torch.utils.data.TensorDataset(X)\n        dataloader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n        for epoch in range(self.num_epochs):\n            for i, inputs in enumerate(dataloader):\n                self.optimizer.zero_grad()\n                inputs = inputs[0]\n                outputs = self(inputs)\n                loss = self.criterion(outputs, inputs)\n                self.losses.append(loss.item())\n                loss.backward()\n                self.optimizer.step()\n        self.is_trained = True\n\n    def _build_encoder(self, layers, activation):\n        encoder_layers = []\n        for i in range(1, len(layers)):\n            encoder_layers.append(nn.Linear(layers[i - 1], layers[i]))\n            encoder_layers.append(self._activation_functions[activation])\n        return nn.Sequential(*encoder_layers)\n\n    def _build_decoder(self, layers, activation):\n        decoder_layers = []\n        for i in range(1, len(layers)):\n            decoder_layers.append(nn.Linear(layers[i - 1], layers[i]))\n            # Don't append activation function for output layer\n            if i &lt; len(layers) - 1:\n                decoder_layers.append(self._activation_functions[activation])\n        return nn.Sequential(*decoder_layers)\n\n    def _prepare_data(self, X):\n        \"\"\"\n        Scales (if necessary) the data and places it afterwards on device\n        \"\"\"\n        if self.retrain or not self.is_trained:\n            X = self.scaler.fit_transform(X)\n        else:\n            X = self.scaler.transform(X)\n        return torch.tensor(X, dtype=torch.float32).to(self._device)\n\n    def predict(self, X):\n        X = self._prepare_data(X)\n\n        if self.retrain or not self.is_trained:\n            self._build_model(X.shape[1])\n            self._train(X)\n\n        with torch.no_grad():\n            latent_space = self.encoder(X).detach().cpu().numpy()\n        return self.detector.predict(latent_space)\n</code></pre>"},{"location":"api.html#driftbench.drift_detection.detectors.ClusterDetector","title":"<code>ClusterDetector</code>","text":"<p>               Bases: <code>Detector</code></p> <p>Cluster based drift detector.</p> Source code in <code>driftbench/drift_detection/detectors.py</code> <pre><code>class ClusterDetector(Detector):\n    \"\"\"Cluster based drift detector.\"\"\"\n    supported_methods = [\"kmeans\", \"gaussian mixture\"]\n\n    hparams = ['method', 'n_centers']\n\n    def __init__(self, n_centers, method=\"kmeans\", random_state=42):\n        if not self._validate_cluster_method(method):\n            raise ValueError(\n                f\"Unknown method {method}: Supported cluster methods are {ClusterDetector.supported_methods}.\")\n        self.n_centers = n_centers\n        self.method = method\n        self.random_state = random_state\n\n    def _validate_cluster_method(self, method):\n        return method in ClusterDetector.supported_methods\n\n    def predict(self, X):\n        warnings.simplefilter(\"ignore\")\n        if self.method == \"kmeans\":\n            return np.min(KMeans(n_clusters=self.n_centers, random_state=self.random_state).fit_transform(X), axis=1)\n        elif self.method == \"gaussian mixture\":\n            gm = GaussianMixture(n_components=self.n_centers, random_state=self.random_state)\n            gm.fit(X)\n            return -1.0 * gm.score_samples(X)\n</code></pre>"},{"location":"api.html#driftbench.drift_detection.detectors.Detector","title":"<code>Detector</code>","text":"<p>Detector base class.</p> Source code in <code>driftbench/drift_detection/detectors.py</code> <pre><code>class Detector(metaclass=ABCMeta):\n    \"\"\"Detector base class.\"\"\"\n    hparams = []\n\n    @abstractmethod\n    def predict(self, X):\n        pass\n\n    def evaluate(self, X, y, metric):\n        if not isinstance(metric, Metric):\n            raise ValueError(\"The metric has to be an instance of the metric class.\")\n        prediction = self.predict(X)\n        return metric(prediction, y)\n\n    @property\n    def name(self):\n        return self.__class__.__name__\n\n    def get_hparams(self):\n        return {\n            hp: getattr(self, hp) for hp in self.hparams\n        }\n</code></pre>"},{"location":"api.html#driftbench.drift_detection.detectors.MMDDetector","title":"<code>MMDDetector</code>","text":"<p>               Bases: <code>Detector</code></p> <p>Implementation of MMD algorithm as drift detector based on the Maximum Mean Discrepancy as defined in Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Sch\u00f6lkopf, and Alexander Smola. A kernel two-sample test. Journal of Machine Learning Research, 13(Mar):723\u2013773, 2012. This implementation is based on the blog post of Onur Tunali in https://www.onurtunali.com/ml/2019/03/08/maximum-mean-discrepancy-in-machine-learning.html</p> Source code in <code>driftbench/drift_detection/detectors.py</code> <pre><code>class MMDDetector(Detector):\n    \"\"\"\n    Implementation of MMD algorithm as drift detector based on the Maximum Mean Discrepancy as defined in\n    Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Sch\u00f6lkopf, and Alexander Smola.\n    A kernel two-sample test.\n    Journal of Machine Learning Research, 13(Mar):723\u2013773, 2012.\n    This implementation is based on the blog post of Onur Tunali in\n    https://www.onurtunali.com/ml/2019/03/08/maximum-mean-discrepancy-in-machine-learning.html\n    \"\"\"\n    _device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\n    def __init__(self, window_size, stat_size, offset, kernel=\"multiscale\"):\n        self.window_size = window_size\n        self.stat_size = stat_size\n        self.offset = offset\n        self.kernel = kernel\n\n    def _mmd_score(self, P, Q, kernel):\n        P = torch.from_numpy(P).to(self._device)\n        Q = torch.from_numpy(Q).to(self._device)\n        xx, yy, zz = torch.mm(P, P.t()), torch.mm(Q, Q.t()), torch.mm(P, Q.t())\n        rx = (xx.diag().unsqueeze(0).expand_as(xx))\n        ry = (yy.diag().unsqueeze(0).expand_as(yy))\n\n        dxx = rx.t() + rx - 2. * xx  # Used for A in (1)\n        dyy = ry.t() + ry - 2. * yy  # Used for B in (1)\n        dxy = rx.t() + ry - 2. * zz  # Used for C in (1)\n\n        XX, YY, XY = (torch.zeros(xx.shape).to(self._device),\n                      torch.zeros(xx.shape).to(self._device),\n                      torch.zeros(xx.shape).to(self._device))\n\n        if kernel == \"multiscale\":\n\n            bandwidth_range = [0.2, 0.5, 0.9, 1.3]\n            for a in bandwidth_range:\n                XX += a ** 2 * (a ** 2 + dxx) ** -1\n                YY += a ** 2 * (a ** 2 + dyy) ** -1\n                XY += a ** 2 * (a ** 2 + dxy) ** -1\n\n        if kernel == \"rbf\":\n\n            bandwidth_range = [10, 15, 20, 50]\n            for a in bandwidth_range:\n                XX += torch.exp(-0.5 * dxx / a)\n                YY += torch.exp(-0.5 * dyy / a)\n                XY += torch.exp(-0.5 * dxy / a)\n\n        return torch.mean(XX + YY - 2. * XY)\n\n    def predict(self, X):\n        N = X.shape[0]\n        prediction = np.full(N, np.nan)\n        # Store last calculated score for the data batch containing not enough data.\n        last_score = 1.\n        for i in range(N - self.stat_size + 1):\n            # Break if data window doesn't have enough data for next window anymore\n            if i + self.offset + self.window_size &gt; N:\n                break\n            stat_batch = X[i:i + self.stat_size]\n            data_batch = X[i + self.offset :i + self.offset + self.window_size]\n            score = self._mmd_score(stat_batch, data_batch, kernel=self.kernel)\n            prediction[i + self.window_size - 1] = score.detach().cpu().item()\n            last_score = score\n        prediction = np.nan_to_num(prediction, nan=last_score.detach().cpu().item())\n        return prediction\n</code></pre>"},{"location":"api.html#driftbench.drift_detection.detectors.RandomGuessDetector","title":"<code>RandomGuessDetector</code>","text":"<p>               Bases: <code>Detector</code></p> <p>A random guess detector.</p> Source code in <code>driftbench/drift_detection/detectors.py</code> <pre><code>class RandomGuessDetector(Detector):\n    \"\"\"A random guess detector.\"\"\"\n    def __init__(self, random_seed=42):\n        self.rng = np.random.RandomState(random_seed)\n\n    def predict(self, X):\n        N = X.shape[0]\n        scores = self.rng.normal(size=N)\n        scores = np.cumsum(scores)\n        return scores\n</code></pre>"},{"location":"api.html#driftbench.drift_detection.detectors.RollingMeanDifferenceDetector","title":"<code>RollingMeanDifferenceDetector</code>","text":"<p>               Bases: <code>Detector</code></p> <p>Calculates the maximum value over a rolling mean across time and returns the absolute difference between subsequent steps.</p> Source code in <code>driftbench/drift_detection/detectors.py</code> <pre><code>class RollingMeanDifferenceDetector(Detector):\n    \"\"\"Calculates the maximum value over a rolling mean across time and returns the absolute\n    difference between subsequent steps.\"\"\"\n\n    hparams = ['window_size', 'center', 'fillna_strategy']\n\n    def __init__(self, window_size, center=False, fillna_strategy=None):\n        self.window_size = window_size\n        self.center = center\n        self.fillna_strategy = fillna_strategy\n\n    def predict(self, X):\n        series_data = pd.DataFrame(X)\n        prediction = series_data.rolling(self.window_size, center=self.center).mean().max(axis=1).diff().abs()\n        if self.fillna_strategy:\n            fill_value = self.fillna_strategy(prediction)\n            prediction = prediction.fillna(fill_value)\n        return prediction.values\n</code></pre>"},{"location":"api.html#driftbench.drift_detection.detectors.RollingMeanStandardDeviationDetector","title":"<code>RollingMeanStandardDeviationDetector</code>","text":"<p>               Bases: <code>Detector</code></p> <p>Detector that applies a rolling mean followed by a rolling  standard deviation and returns the result as the drift score.</p> Source code in <code>driftbench/drift_detection/detectors.py</code> <pre><code>class RollingMeanStandardDeviationDetector(Detector):\n    \"\"\"Detector that applies a rolling mean followed by a rolling \n    standard deviation and returns the result as the drift score.\"\"\"\n    def __init__(self, window_size, center=False, fillna_strategy=None):\n        self.window_size = window_size\n        self.center = center\n        self.fillna_strategy = fillna_strategy\n\n    def predict(self, X):\n        df_data = pd.DataFrame(X)\n        prediction = df_data.rolling(self.window_size, center=self.center).mean().max(axis=1).rolling(\n            self.window_size).std()\n        if self.fillna_strategy:\n            fill_value = self.fillna_strategy(prediction)\n            prediction = prediction.fillna(fill_value)\n        return prediction.values\n</code></pre>"},{"location":"api.html#driftbench.drift_detection.detectors.SlidingKSWINDetector","title":"<code>SlidingKSWINDetector</code>","text":"<p>               Bases: <code>Detector</code></p> <p>Detector based on KS-test.</p> Source code in <code>driftbench/drift_detection/detectors.py</code> <pre><code>class SlidingKSWINDetector(Detector):\n    \"\"\"Detector based on KS-test.\"\"\"\n    def __init__(self, window_size, stat_size, offset):\n        self.window_size = window_size\n        self.stat_size = stat_size\n        self.offset = offset\n\n    def predict(self, X):\n        N = X.shape[0]\n        prediction = np.ones((N,))\n        stat_batches = np.array([X[i:i + self.stat_size] for i in range(N - self.stat_size + 1)])\n        data_batches = np.array([X[i:i + self.window_size] for i in range(self.offset, N - self.window_size + 1)])\n        num_batches = np.min([stat_batches.shape[0], data_batches.shape[0]])\n        # Store last calculated score for the data batch containing not enough data.\n        last_score = 1.\n        for i in range(num_batches):\n            stat_batch, data_batch = stat_batches[i], data_batches[i]\n            _, p_value = ks_2samp(stat_batch, data_batch, method=\"auto\")\n            score = np.log1p(1.0 / p_value)\n            prediction[i + self.window_size - 1] = score\n            last_score = score\n        prediction[:self.window_size] = last_score\n        return prediction\n</code></pre>"},{"location":"api.html#driftbench.drift_detection.metrics.AUC","title":"<code>AUC</code>","text":"<p>               Bases: <code>Metric</code></p> <p>The area under the curve.</p> Source code in <code>driftbench/drift_detection/metrics.py</code> <pre><code>class AUC(Metric):\n    \"\"\"The area under the curve.\"\"\"\n\n    def __call__(self, prediction, target):\n        prediction = np.nan_to_num(prediction, 0)\n        return metrics.roc_auc_score(target, prediction)\n</code></pre>"},{"location":"api.html#driftbench.drift_detection.metrics.SoftTAUC","title":"<code>SoftTAUC</code>","text":"<p>               Bases: <code>Metric</code></p> <p>A softened version of the TAUC.</p> Source code in <code>driftbench/drift_detection/metrics.py</code> <pre><code>class SoftTAUC(Metric):\n    \"\"\"A softened version of the TAUC.\"\"\"\n    _supported_integration_rules = [\"step\", \"trapez\"]\n\n    def __init__(self, rule=\"step\"):\n        self.rule = rule\n\n    def __call__(self, prediction, targets, return_scores=False):\n        overlap_score = SoftOverlapScore()\n        thresholds = np.unique(prediction)\n        thresholds = np.append(thresholds, np.inf)\n        thresholds.sort()\n        scores = np.zeros((thresholds.shape[0], 2))\n        for i, threshold in enumerate(thresholds):\n            bin_predictions = (prediction &gt;= threshold).astype(int)\n            fpr = (bin_predictions[targets == 0] == 1).sum() / (targets == 0).sum()\n            scores[i] = [overlap_score(bin_predictions, targets), fpr]\n        os, fpr = scores[:, 0], scores[:, 1]\n\n        if return_scores:\n            return thresholds, fpr, os\n\n        if self.rule == \"trapez\":\n            return metrics.auc(fpr, os)\n        elif self.rule == \"step\":\n            return np.sum(np.diff(fpr[::-1]) * os[::-1][:-1])\n</code></pre>"},{"location":"api.html#driftbench.drift_detection.metrics.TemporalAUC","title":"<code>TemporalAUC</code>","text":"<p>               Bases: <code>Metric</code></p> <p>The temporal area under the curve.</p> Source code in <code>driftbench/drift_detection/metrics.py</code> <pre><code>class TemporalAUC(Metric):\n    \"\"\"The temporal area under the curve.\"\"\"\n    _supported_integration_rules = [\"step\", \"trapez\"]\n\n    def __init__(self, rule=\"step\"):\n        if not self._is_valid_rule(rule):\n            raise ValueError(\n                f\"Unknown rule {rule}: Supported integration rules are {TemporalAUC._supported_integration_rules}.\")\n        self.rule = rule\n\n    def _is_valid_rule(self, rule):\n        return rule in TemporalAUC._supported_integration_rules\n\n    def __call__(self, prediction, targets, return_scores=False):\n        overlap_score = OverlapScore()\n        thresholds = np.unique(prediction)\n        thresholds = np.append(thresholds, np.inf)\n        thresholds.sort()\n        scores = np.zeros((thresholds.shape[0], 2))\n        for i, threshold in enumerate(thresholds):\n            bin_predictions = (prediction &gt;= threshold).astype(int)\n            fpr = (bin_predictions[targets == 0] == 1).sum() / (targets == 0).sum()\n            scores[i] = [overlap_score(bin_predictions, targets), fpr]\n        os, fpr = scores[:, 0], scores[:, 1]\n\n        if return_scores:\n            return thresholds, fpr, os\n\n        if self.rule == \"trapez\":\n            return metrics.auc(fpr, os)\n        elif self.rule == \"step\":\n            return np.sum(np.diff(fpr[::-1]) * os[::-1][:-1])\n\n    @property\n    def name(self):\n        return f'TAUC-{self.rule}'\n</code></pre>"},{"location":"api.html#benchmarks","title":"Benchmarks","text":""},{"location":"api.html#driftbench.benchmarks.data.Dataset","title":"<code>Dataset</code>","text":"<p>Represents a container class for a dataset specification for benchmarking purposes.</p> Source code in <code>driftbench/benchmarks/data.py</code> <pre><code>class Dataset:\n    \"\"\"\n    Represents a container class for a dataset specification for benchmarking purposes.\n    \"\"\"\n\n    def __init__(self, name, spec, f=None, w0=None, n_variations=5):\n        \"\"\"\n        Args:\n            name (str): The name of the dataset specification.\n            spec (dict): The yaml-specification of the dataset.\n            f (Callable): The function to fit the curves.\n            w0 (np.ndarray): The inital value for the internal parameters.\n            n_variations (int): The number of variations each dataset is sampled.\n            Each dataset is sampled as many times as `n_variations` is set, each time with a\n            different random seed.\n        \"\"\"\n        self.spec = spec\n        self.name = name\n        self.n_variations = n_variations\n        self.w0 = w0\n        self.f = f\n\n        drift_bounds = self.spec[\"drifts\"].get_individual_drift_bounds()\n        self.Y = transform_drift_segments_into_binary(drift_bounds, self.spec[\"N\"])\n\n    def _generate(self, random_state):\n        _, _, curves = sample_curves(\n            dataset_specification=self.spec,\n            f=self.f,\n            w0=self.w0,\n            random_state=random_state,\n        )\n        return curves\n\n    def __iter__(self):\n        for i in range(self.n_variations):\n            X = self._generate(random_state=i)\n            yield i, X, self.Y\n</code></pre>"},{"location":"api.html#driftbench.benchmarks.data.Dataset.__init__","title":"<code>__init__(name, spec, f=None, w0=None, n_variations=5)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the dataset specification.</p> required <code>spec</code> <code>dict</code> <p>The yaml-specification of the dataset.</p> required <code>f</code> <code>Callable</code> <p>The function to fit the curves.</p> <code>None</code> <code>w0</code> <code>ndarray</code> <p>The inital value for the internal parameters.</p> <code>None</code> <code>n_variations</code> <code>int</code> <p>The number of variations each dataset is sampled.</p> <code>5</code> Source code in <code>driftbench/benchmarks/data.py</code> <pre><code>def __init__(self, name, spec, f=None, w0=None, n_variations=5):\n    \"\"\"\n    Args:\n        name (str): The name of the dataset specification.\n        spec (dict): The yaml-specification of the dataset.\n        f (Callable): The function to fit the curves.\n        w0 (np.ndarray): The inital value for the internal parameters.\n        n_variations (int): The number of variations each dataset is sampled.\n        Each dataset is sampled as many times as `n_variations` is set, each time with a\n        different random seed.\n    \"\"\"\n    self.spec = spec\n    self.name = name\n    self.n_variations = n_variations\n    self.w0 = w0\n    self.f = f\n\n    drift_bounds = self.spec[\"drifts\"].get_individual_drift_bounds()\n    self.Y = transform_drift_segments_into_binary(drift_bounds, self.spec[\"N\"])\n</code></pre>"},{"location":"benchmarking.html","title":"An example benchmark pipeline","text":"<p>Here, we explain the ingredients for a benchmarking pipeline build with <code>driftbench</code>. Roughly speaking, the following is needed:</p> <ul> <li>A list of detectors from <code>driftbench.drift_detection.detectors</code> as explained here.</li> <li>One or more metrics from <code>driftbench.drift_detection.metrics</code> as explained here.</li> <li>A dataset specification as explained here.</li> </ul> <p>First, we set up a list of detectors we would like to benchmark:</p> <pre><code>from driftbench.drift_detection.detectors import (\n    ClusterDetector,\n    AutoencoderDetector,\n    SlidingKSWINDetector,\n    MMDDetector,\n)\n\ndetectors = [\n    ClusterDetector(n_centers=5, method=\"gaussian mixture\"),\n    AutoencoderDetector(\n        hidden_layers=[80, 20, 4],\n        retrain_always=True,\n        detector=AggregateFeatureAlgorithm(\n            algorithm=SlidingKSWINDetector(window_size=20, stat_size=20, offset=10),\n        ),\n        num_epochs=10,\n        batch_size=200,\n        lr=0.0001,\n    ),\n    AutoencoderDetector(\n        hidden_layers=[80, 20, 4],\n        retrain_always=True,\n        detector=MMDDetector(window_size=20, stat_size=20, offset=10),\n        num_epochs=10,\n        batch_size=200,\n        lr=0.0001,\n    )\n]\n</code></pre> <p>Next, we set up the dataset we would like to benchmark on.</p> <pre><code>from driftbench.data_generation.loaders import load_dataset_specification_from_yaml\nfrom driftbench.benchmarks.data import Dataset\n\nwith open(\"/path/to/your/spec.yml\", 'r') as f:\n    data_spec = load_dataset_specification_from_yaml(f)\n\ndataset = Dataset(\n    name=\"dataset-1\",\n    spec=data_spec,\n    n_variations=5,\n) \n</code></pre> <p>Finally, we specify which metrics should be tested:</p> <pre><code>from driftbench.drift_detection.metrics import TemporalAUC, AUC\nmetrics = [TemporalAUC(rule='step'), AUC()]\n</code></pre> <p>Finally, we test and evaluate all detectors on all datasets for all metrics:</p> <pre><code>for dataset in datasets:\n    for variation, X, Y in dataset:\n        logger.info(f'Evaluate dataset {dataset.name}: {variation+1}/{dataset.n_variations}')\n        for detector in detectors:\n            prediction = detector.predict(X)\n\n            data = {\n                'dataset_name': dataset.name,\n                'variation': variation,\n                'detector_name': detector.name,\n                'hparams': detector.get_hparams(),\n                'prediction': prediction.tolist(),\n                'ground_truth': Y.tolist(),\n            }\n\n            for metric_fn in metrics:\n                score = metric_fn(prediction, Y)\n                print(\n                    f\"Detector {detector.name} got {score} ({metrics_fn.name}) \"\n                    f\"on dataset variation {variation}\"\n                )\n</code></pre>"},{"location":"data.html","title":"Data Generation","text":"<p>In order to create process curves with a realistic behaviour of process drifts, <code>driftbench</code> synthesizes curves by solving nonlinear optimization problems. By defining a function \\(f(w(t), t)\\)  and support points \\(x\\) and \\(y\\), we can solve for the internal parameters \\(w(t)\\) such that  all the conditions given by the support points are satisfied. The schema is explained in the following section.</p> <p></p>"},{"location":"data.html#synthetization","title":"Synthetization","text":"<p>In the first step, we need to define latent information which encodes the shape of the curves to synthesize. This is done by formulating such a spec in a <code>yaml</code>-file. For example, the following spec defines a polynomial of 7-th degree: <pre><code>example:\n  N: 10000\n  dimensions: 100\n  x_scale: 0.2 \n  y_scale: 0.2\n  func: w[7]* x**7 + w[6]* x**6 + w[5]* x**5 +w[4]* x**4 + w[3] * x**3 + w[2] * x**2 + w[1] * x + w[0]\n  w_init: np.zeros(8)\n  latent_information:\n    !LatentInformation\n    x0: [0, 1, 3, 2, 4]\n    y0: [0, 4, 7, 5, 0]\n    x1: [1, 3]\n    y1: [0, 0]\n    x2: [1]\n    y2: [0]\n  drifts:\n    !DriftSequence\n      - !LinearDrift\n        start: 1000\n        end: 1100\n        feature: y0     \n        dimension: 2    \n        m: 0.002\n</code></pre> The root key defines the name of the dataset, in this case <code>example</code>. The other keys of this <code>yaml</code> structure are:</p> <ul> <li><code>N</code>: The number of curves to synthesize.</li> <li><code>dimensions</code>: The number of timesteps one curve consists of.</li> <li><code>x_scale</code>: The scale of a random gaussian noise which is applied to the <code>x</code>-latent information. If set to 0, no scale noise is applied.</li> <li><code>y_scale</code>: The scale of a random gaussian noise which is applied to the <code>y</code>-latent information. If set to 0, no scale noise is applied.</li> <li><code>func</code>: The function which defines the shape of a curve. The internal parameters are denoted as <code>w</code>, while the timesteps which are used to evaluate the curve are denoted as <code>x</code>.</li> <li><code>w_init</code>: The initial guess for the internal parameters. Must match the number of internal parameters defined in <code>func</code>.</li> <li><code>latent_information</code>: Contains a <code>LatentInformation</code> structure, which holds the latent information which defines the support points of the curves. The <code>x_i</code>denote the <code>x</code>-information for the <code>i</code>-th derivative of <code>func</code>, while the <code>y_i</code>denote the <code>y</code>-information respectively.</li> <li><code>drifts</code>: Contains a <code>DriftSequence</code> structure, which in turn holds a list of drifts, for example <code>LinearDrift</code>-structures. These drifts are applied in the specified manner on the latent  information for each timestep defined as <code>start</code> as <code>end</code> within the <code>N</code> curves. The drift structure  defines the <code>feature</code> and the <code>dimension</code> as well as internal parameters, like in this case  the slope <code>m</code>.</li> </ul> <p>After setting up such a specification, you can call the <code>sample_curves</code>-function, and retrieve the coefficients, respective latent information and curve for each timestep. <pre><code>coefficients, latent_information, curves = sample_curves(dataset[\"example\"], measurement_scale=0.1)\n</code></pre> By specifying a value for <code>measurement_scale</code> some gaussian noise with the specified scale is applied on each value for every curve. By default, \\(5\\%\\) of the mean of the curves is used. If you want to omit the scale, set it to <code>0.0</code> explicitly.</p>"},{"location":"detectors.html","title":"Detectors","text":"<p>Generally, the detection paradigm of <code>driftbench</code> follows the scheme illustrated below. First, features are extracted from each curve, and then aggregations are performed over the temporal axis (often corresponding to process executions) in the resulting latent space.</p> <p></p>"},{"location":"detectors.html#overview","title":"Overview","text":"<p>This module provides a collection of drift detectors, ranging from simple baselines to sophisticated statistical and neural approaches. All detectors inherit from a common  <code>Detector</code> interface. Here a short overview:</p> <ul> <li><code>SlidingKSWINDetector</code>:  Implements    a sliding-window Kolmogorov\u2013Smirnov test to compare distributions of past and recent data    windows. The size of the windows as well as the offset in between can be parameterized.</li> <li><code>MMDDetector</code>: Applies the Maximum Mean    Discrepancy (MMD) test to compare two samples using kernel methods (multiscale or RBF).</li> <li><code>AggregateFeatureAlgorithm</code>:    Aggregates temporal data (e.g., by computing the mean across time) and applies another drift    detector to the result which can be set via <code>algorithm</code> parameter.</li> <li><code>ClusterDetector</code>: Applies clustering    (either KMeans or Gaussian Mixture Model) to the data and uses distance to clusters or    likelihoods as drift scores.</li> <li><code>AutoencoderDetector</code>:  Uses a neural    autoencoder to reduce dimensionality of the data, then applies another detector in the latent    space. Can retrain on each call or reuse the trained model. Hidden layers, activation functions    and learning rate can be set via parameters as well as the downstream detector (via parameter    <code>detector</code>).</li> <li><code>RollingMeanDifferenceDetector</code>:    Calculates the maximum value over a rolling mean across time and returns the absolute difference    between subsequent steps.</li> <li><code>RandomGuessDetector</code>: Outputs a    cumulative sum of random Gaussian noise. Acts as a simple baseline for drift detection.</li> <li><code>RollingMeanStandardDeviationDetector</code>:    Applies a rolling mean followed by a rolling standard deviation and returns the result as the    drift score.</li> <li><code>AlwaysGuessDriftDetector</code>: A    trivial detector that always predicts drift. Useful for debugging or as a degenerate upper bound.    Provides a simple baseline.</li> </ul>"},{"location":"how_it_works.html","title":"Mathematical model","text":""},{"location":"how_it_works.html#technical-implementation-in-jax","title":"Technical implementation in <code>jax</code>","text":"<p>This package uses [JAX](JAX (https://github.com/jax-ml/jax) as its backend for generating synthetic curves. In particular, <code>jax</code> is used for:</p> <ul> <li>Solving non-linear optimization problems.</li> <li>Automatic differentiation for calculating partial derivates of arbitrary functions.</li> <li>Just-in-time (JIT)-compilation with XLA for performance optimization.</li> </ul> <p>For more detailed information regarding the XLA-compilation, please see the  offical JAX documentation or XLA-documentation.</p> <p>These three points ensure an efficient generation of curves, while being able to control the latent information used and the behaviour of drifts applied on the curves.</p> <p>The method used to solve optimization problems is the  LBFGS algorithm, which is supported by <code>jax</code>. The corresponding functions in order to compute the error term, running a iteration of the optimization solving problem, and computing the gradients is all done in functions which are  compiled just-in-time and can be run on a GPU. The procedure can be described as follows:</p> <ul> <li>Choose a function \\(f(w(t), x)\\), which describes the shape of the curves to generate with  inital internal parameters \\(w_0(t)\\).</li> <li>Provide problem constraints encoded in <code>LatentInformation</code> objects.</li> <li>Compute the partial derivates of \\(f(w(t), x)\\) with respect to \\(x_i\\).</li> <li>For each latent information object, compute \\(w(t)\\) according to the LBFGS-algorithm.</li> <li>Return all computed solutions \\(w(t)\\) for each curve.</li> </ul>"}]}